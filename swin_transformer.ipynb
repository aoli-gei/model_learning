{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer\n",
    "自己制造一个 *Swin Transformer* ，当然最终的目的是想把它改造成 U-Net 的架构，我希望对它进行如下改进：\n",
    "1. 去除相对位置编码：我会在进行 qkv 映射的时候使用卷积，引入卷积的归纳偏置，看看是否可以去除相对位置编码问题\n",
    "2. 去除滑动窗口：因为使用了 U-Net 架构以及卷积，滑动窗口显得不是很必要\n",
    "\n",
    "> 暂时就想到了这些，之后有什么东西想到再加上去吧，也算锻炼一下自己的复现能力\n",
    "> 但是这个还叫 Swin 吗？叫做 window Transformer 好了，没有滑动，只有窗口自注意力"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 预处理\n",
    "最基本的部分，包含 embedding，patch 划分，patch 还原，窗口划分，窗口复原"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 embedding\n",
    "对图像进行映射，准备转化为 Token 以便交给 Transformer 作处理\n",
    "可以选择是使用线性映射还是卷积，映射完之后自动转换成 Token 的 2d 表示，以便于之后的窗口划分\n",
    "1. embedding 并划分 patch\n",
    "\n",
    "> 后面思考了一下，既然要下采样，不如还是每一个像素当成一个 Token，256*256 也不是一个太大的分辨率，直接上吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "class patchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "        进行 embedding 以及 patch 划分\n",
    "        Args:\n",
    "            inChannels: input image channels\n",
    "            embedDim: embedding dim\n",
    "            methods: how to do embedding, conv or linear\n",
    "        return:\n",
    "            x: (B,H,W,C), H & W is patch's height & width\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, emb_dim, methods):\n",
    "        super().__init__()\n",
    "        self.methods = methods\n",
    "        if methods == 'conv':\n",
    "            self.proj = nn.Conv2d(in_channels, emb_dim, 3, 1, 1)\n",
    "        elif methods == 'linear':\n",
    "            self.proj = nn.Linear(in_channels, emb_dim)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        if self.methods == 'conv':\n",
    "            x = self.proj(x)\n",
    "            x = rearrange(x, 'b c (h ph) (w pw) -> b h w (ph pw c)', ph=1, pw=1)\n",
    "        elif self.methods == 'linear':\n",
    "            x = rearrange(x, 'b c (h ph) (w pw) -> b h w (ph pw c)', ph=1, pw=1)\n",
    "            x = self.proj(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256, 16])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "pe = patchEmbedding(3, 16, 'conv')\n",
    "test = torch.randn(1, 3, 256, 256)\n",
    "pe(test).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 窗口划分\n",
    "将生成的 2d patch 进行窗口划分，以便于使用窗口自注意力\n",
    "1. 划分窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowPartition(x, window_size):\n",
    "    \"\"\"\n",
    "        Window partition, based on patch\n",
    "        Args:\n",
    "            windowSize: The size of the windos, based on patch\n",
    "        returns:\n",
    "            (B*numWindows, Wh, Ww, C), C is a Token\n",
    "    \"\"\"\n",
    "    x = rearrange(x, 'b (h wh) (w ww) c -> (b h w) wh ww c', wh=window_size, ww=window_size)   # B*numWindows windowHeight windowWidth C\n",
    "    return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 划分窗口还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowReverse(x, resolution):\n",
    "    \"\"\"\n",
    "        Window reverse, reversing the window partition back to patch\n",
    "        Args:\n",
    "            imageSize: The size of the image, based on pixels\n",
    "        return:\n",
    "            (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B, Wh, Ww, _ = x.shape\n",
    "    ratio = resolution//Wh\n",
    "    x = rearrange(x, '(b h w) wh ww c -> b (h wh) (w ww) c', h=ratio, w=ratio)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "         [[True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          ...,\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True],\n",
       "          [True, True, True,  ..., True, True, True]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "test = torch.randn(1, 256, 256, 16)\n",
    "test_win = windowPartition(test, 8)\n",
    "test_rev = windowReverse(test_win, 256)\n",
    "test == test_rev\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型架构\n",
    "\n",
    "这里要开始编写模型的架构细节了，从每一个小结构来构成整个模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 窗口自注意力\n",
    "\n",
    "这里要编写窗口自注意力方法，swin transformer 在窗口自注意力中包含 mask 以及以及相对位置编码，这里打算不使用滑动窗口，但是会保留是否使用相对位置编码的选项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class windowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Self attention based on windows\n",
    "        input: (B*num_windows, N, C)\n",
    "        Args:\n",
    "            patchSize: the size of the patch\n",
    "            patchDim: the dim of the patch, 2d representation\n",
    "            attentionDim: the projection dim of qkv\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size, dim, num_heads, pe_flag):\n",
    "        super().__init__()\n",
    "        self.to_qkv = nn.Conv2d(dim, dim*3, 3, 1, 1)\n",
    "        self.proj = nn.Conv2d(dim, dim, 3, 1, 1)\n",
    "        self.num_heads = num_heads\n",
    "        self.pe_Flag = pe_flag\n",
    "        self.window_size = window_size\n",
    "        head_dim = dim//num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "        if pe_flag:\n",
    "            # define a parameter table of relative position bias\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "            # get pair-wise relative position index for each token inside the window\n",
    "            coords_h = torch.arange(self.window_size)\n",
    "            coords_w = torch.arange(self.window_size)\n",
    "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "            relative_coords[:, :, 0] += self.window_size - 1  # shift to start from 0\n",
    "            relative_coords[:, :, 1] += self.window_size - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
    "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape  # B*num_windows, window_size**2, C\n",
    "        x=rearrange(x,'b (wh ww) c -> b c wh ww',wh=self.window_size,ww=self.window_size)  # 转换为 2d 图像表示\n",
    "        qkv = self.to_qkv(x)    # B*num_windows, C**3, window_size, windowsize\n",
    "        qkv = rearrange(qkv, 'b (num head head_dim) wh ww  -> num b head (wh ww) head_dim',num=3, head=self.num_heads, head_dim=C//self.num_heads) # 转为 token 表示\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q=q*self.scale\n",
    "        attn = (q @ k.transpose(-2,-1))\n",
    "\n",
    "        if self.pe_Flag:\n",
    "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                self.window_size * self.window_size, self.window_size * self.window_size, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)  # positional enbedding\n",
    "        \n",
    "        attn=self.softmax(attn)\n",
    "        x=(attn @ v) #B*num_windows, heads, n, head_dim\n",
    "        x=rearrange(x,'b head (wh ww) head_dim -> b (head head_dim) wh ww ',wh = self.window_size,ww=self.window_size)\n",
    "        x=self.proj(x)  # B*num_window, window_size, window_size, dim\n",
    "        x=rearrange(x,'b c wh ww -> b (wh ww) c')   # B*num_windows, n, c\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testWA = windowAttention(2,16,4,True)  # head=4,head_dim = 4\n",
    "test = torch.randn(1*4,2*2,16)  # 这里的数据符合窗口自注意力的数据，如果想要改变窗口的大小需要另行调整\n",
    "testWA(test).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 不同的映射方式\n",
    "\n",
    "自注意力映射矩阵可以有不同的方式，上面的方式选择的是直接使用标准卷积核做映射，可能会导致参数过多，这里做出一些修改\n",
    "\n",
    "1. 深度可分离卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSconv(nn.Module):\n",
    "    \"\"\"\n",
    "        深度可分离卷积\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels, out_channels, kernel_size,padding,stride,dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.dwconv=nn.Conv2d(in_channels=in_channels,out_channels=in_channels,kernel_size=kernel_size,padding=padding,stride=stride,groups=in_channels)\n",
    "        self.pwconv=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=1,stride=1)\n",
    "        self.drop=nn.Dropout2d(dropout_ratio)\n",
    "    def forward(self,x):\n",
    "        x=self.dwconv(x)\n",
    "        x=self.pwconv(x)\n",
    "        x=self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 降低 KV 数量\n",
    "\n",
    "增大卷积步长来减少 KV 的数量，从而缓解自注意力的运算压力\n",
    "\n",
    "> 这里直接选择深度可分离卷积作为映射方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Self attention based on windows\n",
    "        input: (B*num_windows, N, C)\n",
    "        Args:\n",
    "            patchSize: the size of the patch\n",
    "            patchDim: the dim of the patch, 2d representation\n",
    "            attentionDim: the projection dim of qkv\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size, dim, num_heads, pe_flag,drop_ratio):\n",
    "        super().__init__()\n",
    "        self.to_kv = DSconv(dim, 2*dim, 4, 1, 2)    # 分辨率减小一半\n",
    "        self.to_q = DSconv(dim, dim, 3, 1, 1)  # 标准卷积\n",
    "        self.proj = DSconv(dim, dim, 3, 1, 1)\n",
    "        self.attn_drop = nn.Dropout(drop_ratio)\n",
    "        self.num_heads = num_heads\n",
    "        self.pe_Flag = pe_flag\n",
    "        self.window_size = window_size\n",
    "        head_dim = dim//num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "        if pe_flag:\n",
    "            # define a parameter table of relative position bias\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "            # get pair-wise relative position index for each token inside the window\n",
    "            coords_h = torch.arange(self.window_size)\n",
    "            coords_w = torch.arange(self.window_size)\n",
    "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "            relative_coords[:, :, 0] += self.window_size - 1  # shift to start from 0\n",
    "            relative_coords[:, :, 1] += self.window_size - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
    "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape  # B*num_windows, window_size**2, C\n",
    "        x = rearrange(x, 'b (wh ww) c -> b c wh ww', wh=self.window_size, ww=self.window_size)  # 转换为 2d 图像表示\n",
    "        kv = self.to_kv(x)  # B*num_windows, C**3, window_size, windowsize\n",
    "        q = self.to_q(x)  # B*num_windows, C, window_size, windowsize\n",
    "        kv = rearrange(kv, 'b (num head head_dim) wh ww  -> num b head (wh ww) head_dim',\n",
    "                        num=2, head=self.num_heads, head_dim=C//self.num_heads)  # 转为 token 表示\n",
    "        q =rearrange(q,'b (head head_dim) h w -> b head (h w) head_dim',head=self.num_heads)\n",
    "        k,v=kv[0],kv[1]\n",
    "        q = q*self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        if self.pe_Flag:\n",
    "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                self.window_size * self.window_size, self.window_size * self.window_size, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)  # positional enbedding\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v)  # B*num_windows, heads, n, head_dim\n",
    "        x = rearrange(x, 'b head (wh ww) head_dim -> b (head head_dim) wh ww ', wh=self.window_size, ww=self.window_size)\n",
    "        x = self.proj(x)  # B*num_window, window_size, window_size, dim\n",
    "        x = rearrange(x, 'b c wh ww -> b (wh ww) c')   # B*num_windows, n, c\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2560, -0.0939, -0.0194,  ...,  0.2518,  0.3050, -0.3785],\n",
       "         [-0.2602, -0.1133, -0.0187,  ...,  0.2388,  0.3101, -0.3518],\n",
       "         [-0.2618, -0.1155, -0.0182,  ...,  0.2365,  0.3128, -0.3530],\n",
       "         ...,\n",
       "         [-0.3560, -0.1044,  0.0375,  ...,  0.2084,  0.3182, -0.3383],\n",
       "         [-0.3559, -0.1077,  0.0367,  ...,  0.2072,  0.3186, -0.3393],\n",
       "         [-0.2261, -0.1360,  0.0473,  ...,  0.2014,  0.3205, -0.3506]],\n",
       "\n",
       "        [[-0.2129, -0.0876, -0.0165,  ...,  0.2283,  0.2701, -0.3152],\n",
       "         [-0.1913, -0.1270,  0.0009,  ...,  0.2174,  0.3126, -0.3255],\n",
       "         [-0.1909, -0.1273,  0.0016,  ...,  0.2143,  0.3127, -0.3229],\n",
       "         ...,\n",
       "         [-0.2453, -0.1447,  0.0735,  ...,  0.1842,  0.3495, -0.3479],\n",
       "         [-0.2495, -0.1425,  0.0751,  ...,  0.1821,  0.3498, -0.3451],\n",
       "         [-0.1570, -0.1626,  0.0571,  ...,  0.1950,  0.3388, -0.3490]],\n",
       "\n",
       "        [[-0.2493, -0.0651, -0.0466,  ...,  0.2278,  0.2948, -0.3268],\n",
       "         [-0.2275, -0.0954, -0.0064,  ...,  0.2242,  0.3508, -0.3337],\n",
       "         [-0.2206, -0.1003, -0.0109,  ...,  0.2287,  0.3535, -0.3402],\n",
       "         ...,\n",
       "         [-0.2848, -0.1230,  0.0633,  ...,  0.2266,  0.4055, -0.3977],\n",
       "         [-0.2772, -0.1241,  0.0658,  ...,  0.2252,  0.4161, -0.3997],\n",
       "         [-0.1654, -0.1516,  0.0568,  ...,  0.2034,  0.3921, -0.3639]],\n",
       "\n",
       "        [[-0.1925, -0.0907, -0.0027,  ...,  0.2063,  0.3345, -0.3295],\n",
       "         [-0.1847, -0.1102,  0.0169,  ...,  0.2081,  0.3738, -0.3159],\n",
       "         [-0.1792, -0.1111,  0.0181,  ...,  0.2042,  0.3743, -0.3111],\n",
       "         ...,\n",
       "         [-0.2456, -0.1432,  0.0507,  ...,  0.1569,  0.3984, -0.3420],\n",
       "         [-0.2450, -0.1437,  0.0527,  ...,  0.1578,  0.3952, -0.3399],\n",
       "         [-0.1601, -0.1517,  0.0490,  ...,  0.1742,  0.3917, -0.3330]]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testWA = convWindowAttention(8,16,4,False)  # head=4,head_dim = 4\n",
    "test = torch.randn(1*4,8*8,16)  # 这里的数据符合窗口自注意力的数据，如果想要改变窗口的大小需要另行调整\n",
    "testWA(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 FeedForward\n",
    "这里要处理 FeedForward 结构，感觉有多种选择，一种是像传统的使用全连接层，或者是使用 CNN\n",
    "但是使用 CNN 也会有问题，是要在窗口内使用 CNN 还是在整张图像上使用 CNN\n",
    "\n",
    "> 其实我比较倾向在窗口内使用 CNN，因为既然划分了窗口，那么随着网络的加深降低图像的分辨率同样可以让不同窗口之间的信息进行交互\n",
    "> 但是如果想要实现全局自注意力还是需要让网络再深一层，不然就是随着网络的加深，窗口大小同时增大\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, method, dim, mlp_ratio,dropout_ratio):\n",
    "        \"\"\"\n",
    "            FeedForward Block\n",
    "            Args:\n",
    "                input: (B*num_winodw, N, C)\n",
    "                methods: the projection methods, cnn or mlp\n",
    "                dim: the dim of token\n",
    "                mlp_ratio: the dim of the hidden layer\n",
    "                dropout_ratio: the ratio of the dropout\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.method = method\n",
    "        if method == 'conv':\n",
    "            self.layer1 = nn.Conv2d(dim, dim*mlp_ratio, 3, 1, 1)\n",
    "            self.layer2 = nn.Conv2d(dim*mlp_ratio, dim, 3, 1, 1)\n",
    "            self.drop=nn.Dropout2d(dropout_ratio)\n",
    "        else:\n",
    "            self.layer1 = nn.Linear(dim, dim*mlp_ratio)\n",
    "            self.layer2 = nn.Linear(dim*mlp_ratio, dim)\n",
    "            self.drop=nn.Dropout(dropout_ratio)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.method == 'conv':\n",
    "            B, N, C = x.shape\n",
    "            wh = ww = int(N**0.5)\n",
    "            x = rearrange(x, 'b (wh ww) c -> b c wh ww',wh=wh,ww=ww)\n",
    "            x=self.layer1(x)\n",
    "            x=self.act(x)\n",
    "            x=self.drop(x)\n",
    "            x=self.layer2(x)\n",
    "            x=self.drop(x)\n",
    "            x = rearrange(x, 'b c wh ww -> b (wh ww) c')\n",
    "        else:\n",
    "            x=self.layer1(x)\n",
    "            x=self.act(x)\n",
    "            x=self.drop(x)\n",
    "            x=self.layer2(x)\n",
    "            x=self.drop(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 16])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fw=FeedForward('linear',16,4)\n",
    "test_input=torch.randn(1*4,8*8,16)\n",
    "test_fw(test_input).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Basic Block\n",
    "\n",
    "组成一个基础架构，包含自注意力和 FeedForward \n",
    "对其稍加修改，将 Dropout 添加进去了，据说可以在数据量小的情况下，方式过拟合\n",
    "\n",
    "> 但是有一个问题，我希望在训练的开始阶段使用 Dropout，之后就不使用 Dropout 了，用直接定义对象的方式似乎不是很合适"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        一个基础的 Transformer Block，包含自注意力和 FeedForward\n",
    "        Args:\n",
    "            input: (B*num_windows, N, C)\n",
    "            window_size: the size of the window\n",
    "            dim: Token dim\n",
    "            num_heads: numbers of the heads\n",
    "            pe_flag: use positional encoding or not\n",
    "            mlp_methods: FeedForward methods, Linear or conv\n",
    "            mlp_ratio: the ratio of the feedforward block\n",
    "            drop_ratio: the ratio of the dropout layer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size, dim, num_heads, pe_flag, mlp_method, mlp_ratio, mlp_drop_ratio, attn_drop_ratio):\n",
    "        super().__init__()\n",
    "        self.attn = convWindowAttention(window_size=window_size, dim=dim, num_heads=num_heads, pe_flag=pe_flag, drop_ratio=attn_drop_ratio)\n",
    "        self.feedForward = FeedForward(method=mlp_method, dim=dim, mlp_ratio=mlp_ratio, dropout_ratio=mlp_drop_ratio)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)   # preNorm\n",
    "        x = self.attn(x)\n",
    "        attnout = x+shortcut\n",
    "        mlpout = self.feedForward(attnout)\n",
    "        mlpout = self.norm2(mlpout)\n",
    "        out = mlpout + attnout\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.6063,  0.7361, -0.2138,  ...,  0.3986, -1.8798, -1.1483],\n",
       "         [-1.1505, -0.1727, -1.4844,  ...,  0.1473,  0.0247,  1.5883],\n",
       "         [ 0.9689,  0.7363,  2.8224,  ..., -0.2719, -2.3062,  0.6836],\n",
       "         ...,\n",
       "         [-0.8456, -0.0271,  1.9537,  ...,  1.5990, -1.1831,  0.9799],\n",
       "         [ 1.9941,  1.2018, -1.1034,  ..., -2.2401, -0.4410, -3.7872],\n",
       "         [ 0.9032,  0.8688, -0.6262,  ..., -0.4189, -0.4473,  0.3869]],\n",
       "\n",
       "        [[-2.4947, -0.1487, -2.2262,  ...,  0.1756,  0.6031,  0.5275],\n",
       "         [-0.1865,  0.9018, -1.7052,  ...,  0.7480,  0.3009, -1.6146],\n",
       "         [-0.2341, -0.6768,  0.7450,  ...,  0.2567, -1.1757, -1.1714],\n",
       "         ...,\n",
       "         [-0.2481, -0.1114, -0.1961,  ..., -4.6342,  0.2114, -0.0136],\n",
       "         [-0.7939, -3.1093,  1.7647,  ...,  1.5772, -0.9247,  0.5921],\n",
       "         [ 1.4138, -3.4308, -0.9724,  ..., -1.9892,  0.1002,  0.4153]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testBasicBlock = basicBlock(8,16,2,False,'conv',4,0.1,1)\n",
    "testInput = torch.randn(2,8*8,16)\n",
    "testBasicBlock(testInput)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Basic Layer\n",
    "\n",
    "将上面的块组成 Uformer 中的一层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basicLayer(nn.Module):\n",
    "    \"\"\"\n",
    "        一个基础的 Uformer Layer, 是一层 U-Net 的结构\n",
    "        Args:\n",
    "            input: (B*num_windows, N, C)\n",
    "            window_size: the size of the window\n",
    "            dim: Token dim\n",
    "            num_heads: numbers of the heads\n",
    "            pe_flag: use positional encoding or not\n",
    "            mlp_methods: FeedForward methods, Linear or conv\n",
    "            mlp_ratio: the ratio of the feedforward block\n",
    "            drop_ratio: the ratio of the dropout layer\n",
    "            depth: the depth of this layer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size, dim, num_heads, pe_flag, mlp_method, mlp_ratio, mlp_drop_ratio, attn_drop_ratio, depth):\n",
    "        super().__init__()\n",
    "        self.modlist = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            self.modlist.append(basicBlock(window_size=window_size, dim=dim, num_heads=num_heads, pe_flag=pe_flag,\n",
    "                                mlp_method=mlp_method, mlp_ratio=mlp_ratio, mlp_drop_ratio=mlp_drop_ratio, attn_drop_ratio=attn_drop_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7942, -1.8997, -1.0571,  ...,  1.2608, -0.5754, -2.4363],\n",
       "         [ 0.1233, -1.5293,  1.0393,  ...,  1.8218,  2.8645, -2.0358],\n",
       "         [-2.7585,  0.5277, -0.8782,  ...,  0.8342,  1.9935,  2.6708],\n",
       "         ...,\n",
       "         [ 3.9766,  2.9710, -0.6239,  ..., -1.3399,  3.1986,  3.6853],\n",
       "         [-3.2670,  0.7657, -1.2656,  ..., -0.9717, -0.7699,  0.6301],\n",
       "         [ 1.0382,  2.4210,  1.6876,  ..., -0.1289,  0.3380, -0.9510]],\n",
       "\n",
       "        [[ 1.1781, -2.3276, -4.5955,  ..., -1.2207, -4.9892, -1.4254],\n",
       "         [ 3.0435, -1.1702, -1.1032,  ...,  0.4988,  0.8493,  0.5271],\n",
       "         [-0.9873,  0.2970,  0.6085,  ...,  0.6181, -1.7288,  2.8000],\n",
       "         ...,\n",
       "         [-0.5273, -2.5459,  1.5629,  ..., -0.7672, -0.0823,  0.3997],\n",
       "         [-2.0775,  0.8554, -1.2231,  ..., -2.7632,  2.4376, -0.9827],\n",
       "         [ 1.8451,  1.3500,  0.3397,  ..., -0.9480,  2.7525, -1.2971]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testBasicLayer= basicLayer(8,16,2,False,'conv',4,0.1,0.1,2)\n",
    "testInput = torch.randn(2,8*8,16)\n",
    "testBasicLayer(testInput)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 上下采样\n",
    "\n",
    "使用卷积进行上下采样，以组成 U-Net\n",
    "\n",
    "> 这个上下采样不知道有没有什么骚操作可以做呢~\n",
    "> 刚刚去看了 Uformer 的代码，没有发现有什么可以做的骚操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 下采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dowmSample(nn.Module):\n",
    "    \"\"\"\n",
    "        下采样层，输入请将窗口转换为图像表示\n",
    "        Args:\n",
    "            input: Token, after window partition\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self,dim,window_size,resolution):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.resolution=resolution\n",
    "        self.conv=nn.Conv2d(dim,dim*2,4,2,1)\n",
    "    def forward(self,x):\n",
    "        B,N,C=x.shape\n",
    "        Wh=Ww=int(N**0.5)\n",
    "        x=rearrange(x,'b (Wh Ww) c -> b Wh Ww c',Wh=Wh,Ww=Ww)\n",
    "        x=windowReverse(x,resolution=self.resolution).permute(0,3,1,2).contiguous()   # (B,C,H,W)\n",
    "        x=self.conv(x).permute(0,2,3,1).contiguous()    # (B,H',W',2C), 此时分辨率已减小一半\n",
    "        x=windowPartition(x,window_size=self.window_size)   # (B*num_window, Wh, Ww, C)\n",
    "        x=rearrange(x,'b h w c -> b (h w) c')\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDownSample = dowmSample(16,8,16)    # 注意这个分辨率要写好，不要写歪了\n",
    "test=torch.randn(1*4,8*8,16)\n",
    "testDownSample(test).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 上采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class upSample(nn.Module):\n",
    "    \"\"\"\n",
    "        下采样层，输入请将窗口转换为图像表示\n",
    "    \"\"\"\n",
    "    def __init__(self,dim,window_size,resolution):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.resolution=resolution\n",
    "        self.conv=nn.ConvTranspose2d(dim,dim//2,2,2)\n",
    "    def forward(self,x):\n",
    "        B,N,C=x.shape\n",
    "        Wh=Ww=int(N**0.5)\n",
    "        x=rearrange(x,'b (Wh Ww) c -> b Wh Ww c',Wh=Wh,Ww=Ww)\n",
    "        x=windowReverse(x,resolution=self.resolution).permute(0,3,1,2).contiguous()   # (B,C,H,W)\n",
    "        x=self.conv(x).permute(0,2,3,1).contiguous()    # (B,H',W',2C), 此时分辨率已减小一半\n",
    "        x=windowPartition(x,window_size=self.window_size)   # (B*num_window, Wh, Ww, C)\n",
    "        x=rearrange(x,'b h w c -> b (h w) c')\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 16])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDownSample = dowmSample(16,8,16)    # 注意这个分辨率要写好，不要写歪了\n",
    "test=torch.randn(1*4,8*8,16)\n",
    "down=testDownSample(test)\n",
    "testUpSample=upSample(32,8,8)\n",
    "testUpSample(down).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d75350062c91db8f31525500570c5ecc5362547a35f3bd097b0c9da2ada0503a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

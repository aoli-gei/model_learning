{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "> 尝试自己实现 Transformer 的架构，并提升自己的代码能力  \n",
    "> 参考本仓库：D，当然也可以自己先试一下，有不明白的地方再去研究  \n",
    "> 不对，有一个问题，那为什么我不直接去尝试实现 ViT 呢？都是 Transfomer 架构，并且还更贴近 CV，是我所需要的  \n",
    "> 好吧，转换目标，手撕 ViT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ViT](./ViT.jpg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 预处理部分\n",
    "分为 patch 划分，线性映射以及位置嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class pre_proces(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, patch_dim, dim):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.patch_num = (image_size//patch_size)**2\n",
    "        self.linear_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.patch_num+1, self.dim))  # 使用广播\n",
    "        self.CLS_token = nn.Parameter(torch.randn(1, 1, self.dim))  # 别忘了维度要和 (B,L,C) 对齐\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)  # (B,L,C)\n",
    "        x = self.linear_embedding(x)\n",
    "        b, l, c = x.shape   # 获取 token 的形状 (B,L,c)\n",
    "        CLS_token = repeat(self.CLS_token, '1 1 d -> b 1 d', b=b)  # 位置编码复制 B 份\n",
    "        x = torch.concat((CLS_token, x), dim=1)\n",
    "        x = x+self.position_embedding\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7700,  0.6970, -0.8516,  ...,  1.6258,  1.0387,  1.2238],\n",
       "         [-1.7675, -0.4237,  0.3249,  ..., -0.3300,  1.7388, -0.6130],\n",
       "         [ 1.3180,  1.3085,  0.8253,  ..., -0.0889,  0.6063,  1.9750],\n",
       "         ...,\n",
       "         [-1.1643,  0.4448,  0.3943,  ...,  0.7199, -0.1004, -1.0573],\n",
       "         [-0.3817, -1.6991, -2.5210,  ..., -0.1832,  1.2500,  0.4725],\n",
       "         [ 1.2402,  1.4359,  0.1627,  ..., -0.4160, -0.3571, -0.1215]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pre_proces(128, 16, 768, 768)\n",
    "input = torch.randn(1, 3, 128, 128)\n",
    "p(input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformer Block\n",
    "接下来要构建每一个 Transformer block 了，可以先从一个小块开始"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MultiHead self Attention\n",
    "构建自注意力层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead_self_attention(nn.Module):\n",
    "    def __init__(self, heads, head_dim, dim):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim    # 每一个注意力头的维度\n",
    "        self.heads = heads  # 注意力头个数\n",
    "        self.inner_dim = self.heads*self.head_dim  # 多头自注意力最后的输出维度\n",
    "        self.scale = self.head_dim**-0.5   # 正则化系数\n",
    "        self.to_qkv = nn.Linear(dim, self.inner_dim*3)  # 生成 qkv，每一个矩阵的维度和由自注意力头的维度以及头的个数决定\n",
    "        self.to_output = nn.Linear(self.inner_dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)    # PreNorm\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)  # 划分 QKV，返回一个列表，其中就包含了 QKV\n",
    "        Q, K, V = map(lambda t: rearrange(t, 'b l (h dim) -> b h l dim', dim=self.head_dim), qkv)\n",
    "        K_T = K.transpose(-1, -2)\n",
    "        att_score = Q@K_T*self.scale\n",
    "        att = self.softmax(att_score)\n",
    "        out = att@V   # (B,H,L,dim)\n",
    "        out = rearrange(out, 'b h l dim -> b l (h dim)')  # 拼接\n",
    "        output = self.to_output(out)\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 测试自注意力层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1356,  0.0105,  0.5154,  ...,  0.1114, -0.0470,  0.3496],\n",
       "         [ 0.0756, -0.1498,  0.5160,  ...,  0.0313, -0.0332,  0.2123],\n",
       "         [ 0.1368, -0.0630,  0.4745,  ...,  0.0991, -0.0957,  0.3188],\n",
       "         [ 0.1227, -0.0846,  0.5227,  ...,  0.0340, -0.0341,  0.2845]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MHA = Multihead_self_attention(heads=8, head_dim=64, dim=768)\n",
    "input = torch.randn(1, 4, 768)\n",
    "MHA(input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP\n",
    "构建自注意力层后面的 FeedForward 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 测试 MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = FeedForward(768, 1024)\n",
    "x = torch.randn(1, 4, 768)\n",
    "ff(x).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Transformer block\n",
    "建立残差连接，构建 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_block(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.MHA = Multihead_self_attention(heads=heads, head_dim=head_dim, dim=dim)\n",
    "        self.FeedForward = FeedForward(dim=dim, mlp_dim=mlp_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.MHA(x)+x\n",
    "        x = self.FeedForward(x)+x\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 测试 Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_block = Transformer_block(768,8,64,1024)\n",
    "x=torch.randn(1,4,768)\n",
    "x=transformer_block(x)\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 组装 ViT\n",
    "开始组装 ViT，将上面的各个模块进行整合"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Transformer\n",
    "组成 ViT 的主体部分，也就是 Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, depth):\n",
    "        super().__init__()\n",
    "        self.layers=nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            self.layers.append(Transformer_block(dim=dim,heads=heads,head_dim=head_dim,mlp_dim=mlp_dim))\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 验证组合完成的 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9251,  0.8963,  0.5717,  ..., -1.4688, -0.6899,  2.5642],\n",
       "         [-0.7377,  1.8619,  1.9917,  ..., -0.4586, -0.7757,  0.4690],\n",
       "         [-0.9505, -1.5973,  1.3902,  ..., -0.8011,  0.7190, -0.2873],\n",
       "         [ 1.0755,  1.0339,  2.0910,  ..., -1.0413, -2.8090,  1.7156]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = Transformer(768,8,64,1024,6)\n",
    "transformer(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 ViT\n",
    "构建 ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, channels, patch_size, dim, heads, head_dim, mlp_dim, depth, num_class):\n",
    "        super().__init__()\n",
    "        self.to_patch_embedding = pre_proces(image_size=image_size, patch_size=patch_size, patch_dim=channels*patch_size**2, dim=dim)\n",
    "        self.transformer = Transformer(dim=dim, heads=heads, head_dim=head_dim, mlp_dim=mlp_dim, depth=depth)\n",
    "        self.MLP_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_class)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token = self.to_patch_embedding(x)\n",
    "        output = self.transformer(token)\n",
    "        CLS_token = output[:, 0, :]\n",
    "        out = self.softmax(self.MLP_head(CLS_token))\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 测试 ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1439, 0.2126, 0.4134, 0.2301]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = ViT(image_size=64,channels=3,patch_size=16,dim=768,heads=8,head_dim=64,mlp_dim=1024,depth=6,num_class=4)\n",
    "x=torch.randn(1,3,64,64)\n",
    "vit(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
